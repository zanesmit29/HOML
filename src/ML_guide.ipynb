{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a77b6ff4",
   "metadata": {},
   "source": [
    "# Guide to create a ML Model in Scikit_learn\n",
    "\n",
    "Here is the **original end-to-end scikit-learn guide**—unchanged in structure and comments—plus one new EDA step that visualizes the **correlation between every numeric feature and the target**.\n",
    "\n",
    "***\n",
    "\n",
    "## 1. Set-Up Your Environment\n",
    "\n",
    "- Create and activate a virtual environment (optional but recommended):\n",
    "\n",
    "```bash\n",
    "python -m venv venv && source venv/bin/activate      # macOS / Linux\n",
    "venv\\Scripts\\activate                                 # Windows\n",
    "```\n",
    "\n",
    "- Install the core stack:\n",
    "\n",
    "```bash\n",
    "pip install scikit-learn pandas numpy matplotlib seaborn jupyterlab\n",
    "```\n",
    "\n",
    "    - **scikit-learn** for the ML algorithms\n",
    "    - **pandas / numpy** for data handling\n",
    "    - **matplotlib / seaborn** for visualizations\n",
    "    - **jupyterlab** for an interactive notebook interface\n",
    "\n",
    "***\n",
    "\n",
    "## 2. Import the Libraries\n",
    "\n",
    "In a new notebook or `.py` script:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, classification_report\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "```\n",
    "\n",
    "*Pick additional models or metrics as required.*\n",
    "\n",
    "***\n",
    "\n",
    "## 3. Load the Dataset\n",
    "\n",
    "```python\n",
    "df = pd.read_csv(\"your_data.csv\")      # or pd.read_excel / read_sql / fetch_openml\n",
    "display(df.head())\n",
    "display(df.info())\n",
    "```\n",
    "\n",
    "Checklist:\n",
    "\n",
    "- Check for missing values (`df.isna().sum()`).\n",
    "- Verify data types (`df.dtypes`).\n",
    "- Ensure a **target** column exists.\n",
    "\n",
    "***\n",
    "\n",
    "## 4. Exploratory Data Analysis (EDA) \\& Visualization\n",
    "\n",
    "1. **Univariate plots**\n",
    "\n",
    "```python\n",
    "df['target'].value_counts().plot(kind=\"bar\")         # classification\n",
    "sns.histplot(df['feature'], kde=True)                # regression / classification\n",
    "```\n",
    "\n",
    "2. **Feature-to-target relationships**\n",
    "\n",
    "```python\n",
    "sns.boxplot(x='target', y='numeric_feature', data=df)\n",
    "sns.scatterplot(x='feature1', y='target', data=df)\n",
    "```\n",
    "\n",
    "3. **Pairwise relationships \\& full correlation heatmap**\n",
    "\n",
    "```python\n",
    "sns.pairplot(df.sample(500), hue='target')\n",
    "sns.heatmap(df.corr(numeric_only=True), annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "```\n",
    "\n",
    "4. **Correlation matrix vs. target (NEW)**\n",
    "\n",
    "```python\n",
    "numeric_df  = df.select_dtypes(include=\"number\")\n",
    "corr        = numeric_df.corr()\n",
    "target_corr = corr['target'].drop('target').sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x=target_corr.values, y=target_corr.index, orient='h')\n",
    "plt.title(\"Correlation with Target\")\n",
    "plt.xlabel(\"Pearson r\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "*High |r| values often signal strong predictors; very low |r| values may contribute little. Off-diagonal highs in step 3 hint at multicollinearity.*\n",
    "5. **Missing-value heatmap**\n",
    "\n",
    "```python\n",
    "sns.heatmap(df.isna(), cbar=False)\n",
    "```\n",
    "\n",
    "\n",
    "*Insights from EDA guide preprocessing choices and model selection.*\n",
    "\n",
    "***\n",
    "\n",
    "## 5. Split Data into Train/Test Sets\n",
    "\n",
    "```python\n",
    "X = df.drop(columns=\"target\")\n",
    "y = df[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y if y.nunique() < 20 else None,\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "- Use `stratify` for classification to preserve class proportions.\n",
    "- Keep the test set untouched until final evaluation.\n",
    "\n",
    "***\n",
    "\n",
    "## 6. Preprocessing Pipeline\n",
    "\n",
    "1. **Identify column types**\n",
    "\n",
    "```python\n",
    "num_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "```\n",
    "\n",
    "2. **Create transformers**\n",
    "\n",
    "```python\n",
    "numeric_transformer = Pipeline([\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "```\n",
    "\n",
    "3. **Combine into `ColumnTransformer`**\n",
    "\n",
    "```python\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", numeric_transformer, num_cols),\n",
    "    (\"cat\", categorical_transformer, cat_cols)\n",
    "])\n",
    "```\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "## 7. Build the Model Pipeline\n",
    "\n",
    "```python\n",
    "model = RandomForestClassifier(random_state=42)          # or RandomForestRegressor\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", model)\n",
    "])\n",
    "```\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- End-to-end reproducibility\n",
    "- Prevents data leakage (scaler fits only on training folds)\n",
    "- Seamless hyperparameter tuning with `GridSearchCV`\n",
    "\n",
    "***\n",
    "\n",
    "## 8. Hyperparameter Tuning (Optional but Recommended)\n",
    "\n",
    "```python\n",
    "param_grid = {\n",
    "    \"model__n_estimators\": [100, 300],\n",
    "    \"model__max_depth\": [None, 10, 20],\n",
    "    \"model__min_samples_split\": [2, 5]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipe, param_grid,\n",
    "    cv=5, scoring=\"accuracy\", n_jobs=-1, verbose=2\n",
    ")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "best_model = grid.best_estimator_\n",
    "```\n",
    "\n",
    "- Replace `accuracy` with `neg_root_mean_squared_error` for regression or other metrics.\n",
    "- `best_model` now contains preprocessing + tuned estimator.\n",
    "\n",
    "***\n",
    "\n",
    "## 9. Evaluate on the Test Set\n",
    "\n",
    "```python\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Classification\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Regression\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(f\"Test RMSE: {rmse:.2f}\")\n",
    "```\n",
    "\n",
    "- Inspect confusion matrices or residual plots for deeper insight.\n",
    "\n",
    "***\n",
    "\n",
    "## 10. Persist the Model\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "joblib.dump(best_model, \"model.joblib\")          # Save\n",
    "# later:\n",
    "loaded_model = joblib.load(\"model.joblib\")       # Load\n",
    "```\n",
    "\n",
    "Stores the preprocessing pipeline *and* trained estimator together for seamless deployment.\n",
    "\n",
    "***\n",
    "\n",
    "## 11. Deploy or Integrate\n",
    "\n",
    "- **Batch predictions:** load the joblib file in a scheduled script.\n",
    "- **Real-time API:** wrap the model in a FastAPI or Flask endpoint.\n",
    "- **Edge / mobile:** convert to ONNX using `skl2onnx` if needed.\n",
    "\n",
    "***\n",
    "\n",
    "## 12. Maintain \\& Monitor\n",
    "\n",
    "- Track data and concept drift (e.g., Evidently AI or custom monitoring).\n",
    "- Periodically retrain with fresh data.\n",
    "- Log model predictions \\& feedback for continuous improvement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7eedcf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
